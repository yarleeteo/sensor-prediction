{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onColab = False\n",
    "driveDir = \"google_colab_extension\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import tqdm\n",
    "import dill\n",
    "import pathlib\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "if onColab:\n",
    "    import sys\n",
    "    sys.path.append(f\"/content/drive/MyDrive/{driveDir}\")\n",
    "import D01_1_DataPreparation\n",
    "import Z01_1_General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"LSTM_MHA_01_pytorch_vanila\"\n",
    "\n",
    "if onColab:\n",
    "    # directories\n",
    "    parentDir = f\"/content/drive/MyDrive/{driveDir}/\"\n",
    "else:\n",
    "    parentDir = \"\"\n",
    "\n",
    "rawDataDir = f\"{parentDir}raw_data\"\n",
    "preparedDataDir = f\"{parentDir}prepared_data\"\n",
    "preparedDataPlots = f\"plots/prepared_data\"\n",
    "testValidDataDir = f\"{parentDir}testing_and_validation_data\"\n",
    "modelDir = f\"{parentDir}models/{version}\"\n",
    "scalerDir = f\"scalers/{version}\"\n",
    "lossDir = f\"{parentDir}losses/{version}\"\n",
    "checkpointDir = f\"{parentDir}general_checkpoint/{version}\"\n",
    "learingCurveDir = f\"{parentDir}learning_curve/{version}\"\n",
    "\n",
    "pathlib.Path(f\"{modelDir}/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(f\"{checkpointDir}/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(f\"{lossDir}/\").mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(f\"{learingCurveDir}/\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing progress: 100.00%\r"
     ]
    }
   ],
   "source": [
    "# prepare raw data or no\n",
    "boPrepRawData = True\n",
    "\n",
    "if boPrepRawData:\n",
    "    D01_1_DataPreparation.prepareData(rawDataDir, preparedDataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Generate Datasets ==\n"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "print('== Generate Datasets ==')\n",
    "train_data_files, test_data_files, valid_data_files = D01_1_DataPreparation.splitDatasets(testValidDataDir, preparedDataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data, then scale all together\n",
    "n_steps = 300\n",
    "train_df = pd.DataFrame()\n",
    "for file in train_data_files:\n",
    "    data_path = f\"{preparedDataDir}/sensor data/{file}\"\n",
    "    df, status = D01_1_DataPreparation.loadDataset(data_path, Z01_1_General.DATEFORMAT)\n",
    "    train_df = pd.concat([train_df, df], axis=0)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "for file in test_data_files:\n",
    "    data_path = f\"{preparedDataDir}/sensor data/{file}\"\n",
    "    df, status = D01_1_DataPreparation.loadDataset(data_path, Z01_1_General.DATEFORMAT)\n",
    "    test_df = pd.concat([test_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempCols = [c for c in train_df.columns if re.match('temp', c)]\n",
    "humdCols = [c for c in train_df.columns if re.match('humid', c)]\n",
    "miscCols = [c for c in train_df.columns if not re.match('(temp|humid)', c)]\n",
    "xCols = tempCols + humdCols + miscCols\n",
    "\n",
    "tempScaler = MinMaxScaler(feature_range=(0,1)).fit(train_df.loc[:, tempCols])\n",
    "humdScaler = MinMaxScaler(feature_range=(0,1)).fit(train_df.loc[:, humdCols])\n",
    "miscScaler = MinMaxScaler(feature_range=(0,1)).fit(train_df.loc[:, miscCols])\n",
    "lsScalers = [tempScaler, humdScaler, miscScaler]\n",
    "\n",
    "def scaleData(df, lsScalers):\n",
    "    tempScaler, humdScaler, miscScaler = lsScalers\n",
    "    dfTempCols = pd.DataFrame(tempScaler.transform(df.loc[:, tempCols]), index=df.index, columns=tempCols)\n",
    "    dfHumdCols = pd.DataFrame(humdScaler.transform(df.loc[:, humdCols]), index=df.index, columns=humdCols)\n",
    "    dfMiscCols = pd.DataFrame(miscScaler.transform(df.loc[:, miscCols]))\n",
    "    dfMiscCols.columns = miscCols\n",
    "    dfMiscCols.index = df.index\n",
    "\n",
    "    dfScl = pd.concat([dfTempCols, dfHumdCols, dfMiscCols], axis=1)\n",
    "    return dfScl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for training data\n",
    "# scale and transform data\n",
    "\n",
    "# combine data, then scale all together\n",
    "train_df = pd.DataFrame()\n",
    "for file in train_data_files:\n",
    "    data_path = f\"{preparedDataDir}/sensor data/{file}\"\n",
    "    df, status = D01_1_DataPreparation.loadDataset(data_path, Z01_1_General.DATEFORMAT)\n",
    "    train_df = pd.concat([train_df, df], axis=0)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "for file in test_data_files:\n",
    "    data_path = f\"{preparedDataDir}/sensor data/{file}\"\n",
    "    df, status = D01_1_DataPreparation.loadDataset(data_path, Z01_1_General.DATEFORMAT)\n",
    "    test_df = pd.concat([test_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 300\n",
    "for file in train_data_files:\n",
    "    sensor_df, status = D01_1_DataPreparation.loadDataset(f\"{preparedDataDir}/sensor data/{file}\", Z01_1_General.DATEFORMAT)\n",
    "\n",
    "    # print(len(df), 'before scale from main')\n",
    "    df = scaleData(sensor_df, lsScalers)\n",
    "    # print(len(df), 'after scale from main')\n",
    "\n",
    "    x_data, y_data = D01_1_DataPreparation.transformDataSingleStep(df, n_steps)\n",
    "\n",
    "    if file == train_data_files[0]:\n",
    "        x_train = x_data\n",
    "        y_train = y_data\n",
    "    else:\n",
    "        x_train = np.concatenate([x_train, x_data], axis=0)\n",
    "        y_train = np.concatenate([y_train, y_data], axis=0)\n",
    "\n",
    "for file in test_data_files:\n",
    "    sensor_df, status = D01_1_DataPreparation.loadDataset(f\"{preparedDataDir}/sensor data/{file}\", Z01_1_General.DATEFORMAT)\n",
    "\n",
    "    # print(len(df), 'before scale from main')\n",
    "    df = scaleData(sensor_df, lsScalers)\n",
    "    # print(len(df), 'after scale from main')\n",
    "\n",
    "    x_data, y_data = D01_1_DataPreparation.transformDataSingleStep(df, n_steps)\n",
    "\n",
    "    if file == test_data_files[0]:\n",
    "        x_test = x_data\n",
    "        y_test = y_data\n",
    "    else:\n",
    "        x_test = np.concatenate([x_test, x_data], axis=0)\n",
    "        y_test = np.concatenate([y_test, y_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Description\n",
      "x_train:  (15392, 300, 14) y_train:  (15392, 14)\n",
      "x_test:  (4586, 300, 14) y_test:  (4586, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Description\")\n",
    "print(\"x_train: \", x_train.shape, \"y_train: \", y_train.shape)\n",
    "print(\"x_test: \", x_test.shape, \"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to half, workaround for limited RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliDataset2Half = True\n",
    "if spliDataset2Half:\n",
    "    x_train_1 = x_train[:8000, :]\n",
    "    y_train_1 = y_train[:8000, :]\n",
    "\n",
    "    x_train_2 = x_train[8000:, :]\n",
    "    y_train_2 = y_train[8000:, :]\n",
    "\n",
    "x_train_1 = np.float32(x_train_1)\n",
    "y_train_1 = np.float32(y_train_1)\n",
    "x_train_2 = np.float32(x_train_2)\n",
    "y_train_2 = np.float32(y_train_2)\n",
    "x_test = np.float32(x_test)\n",
    "y_test = np.float32(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "rs = 1\n",
    "torch.manual_seed(rs)\n",
    "\n",
    "dtypeDefault = torch.float32\n",
    "torch.set_default_dtype(dtypeDefault)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # when saving model to deploy, use cpu to avoid troubles\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSet(Dataset):\n",
    "    def __init__(self, xArr, yArr):\n",
    "        self.x = torch.tensor(xArr)\n",
    "        self.y = torch.tensor(yArr)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFeatures = len(train_df.columns)\n",
    "seqLen = len(x_train[0])\n",
    "nHeads = 8\n",
    "nHiddenLSTM = 128\n",
    "nHiddenLinear1 = nHiddenLSTM*seqLen\n",
    "nHiddenLinear2 = 512\n",
    "nHiddenLinear3 = 256\n",
    "batchSz = 16\n",
    "nEpochs = 1000\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmMHAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(nFeatures, nHiddenLSTM, 1, batch_first=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.MHA2 = nn.MultiheadAttention(nHiddenLSTM, nHeads, batch_first=True)\n",
    "        self.linear3 = nn.Linear(nHiddenLinear1, nHiddenLinear2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.linear4 = nn.Linear(nHiddenLinear2, nHiddenLinear3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.linear5 = nn.Linear(nHiddenLinear3, nFeatures)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.MHA2(x, x, x)[0] # take only attn output\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear5(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmMHAModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "lossFn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with first half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first half of dataset first\n",
    "if spliDataset2Half:\n",
    "    xTrainTs = torch.Tensor(x_train_1)\n",
    "    yTrainTs = torch.Tensor(y_train_1)\n",
    "    xTestTs = torch.Tensor(x_test)\n",
    "    yTestTs = torch.Tensor(y_test)\n",
    "else:\n",
    "    xTrainTs = torch.Tensor(x_train)\n",
    "    yTrainTs = torch.Tensor(y_train)\n",
    "    xTestTs = torch.Tensor(x_test)\n",
    "    yTestTs = torch.Tensor(y_test)\n",
    "\n",
    "### Data Loader ###\n",
    "params = {'shuffle': True, 'batch_size': batchSz}\n",
    "\n",
    "if spliDataset2Half:\n",
    "    loaderTrain = DataLoader(DSet(x_train_1, y_train_1), **params)\n",
    "    loaderTest = DataLoader(DSet(x_test, y_test), **params)\n",
    "else:\n",
    "    loaderTrain = DataLoader(DSet(x_train, y_train), **params)\n",
    "    loaderTest = DataLoader(DSet(x_test, y_test), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
     ]
    }
   ],
   "source": [
    "lsLossTr = []\n",
    "lsLossTe = []\n",
    "bestEpoch = 0\n",
    "bestLossTe = 1e6\n",
    "earlyStopThresh = 20\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with tqdm.trange(nEpochs, unit='epoch', mininterval=0) as progBar:\n",
    "\n",
    "    for iEpoch in progBar:\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for xBatchTr, yBatchTr in loaderTrain:\n",
    "            model = model.to(device)\n",
    "            xBatchTr = xBatchTr.to(device)\n",
    "            yBatchTr = yBatchTr.to(device)\n",
    "            yPredBaTr = model(xBatchTr)\n",
    "            lossBaTr = lossFn(yPredBaTr, yBatchTr)\n",
    "            optimizer.zero_grad()\n",
    "            lossBaTr.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation of Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            model = model.to('cpu')\n",
    "            xTrainTs = xTrainTs.to('cpu')\n",
    "            yTrainTs = yTrainTs.to('cpu')\n",
    "            xTestTs = xTestTs.to('cpu')\n",
    "            yTestTs = yTestTs.to('cpu')\n",
    "            yPredTr = model(xTrainTs)\n",
    "            lossTr = np.sqrt(lossFn(yPredTr, yTrainTs))\n",
    "            yPredTe = model(xTestTs)\n",
    "            lossTe = np.sqrt(lossFn(yPredTe, yTestTs))\n",
    "\n",
    "        progBar.set_description(f\"Epoch {iEpoch}: train RMSE {lossTr:.6f}, test RMSE {lossTe:.6f}\")\n",
    "        progBar.update(1)\n",
    "\n",
    "        if iEpoch % 100 == 0:\n",
    "            progBar.write(f\"Epoch {iEpoch}: train RMSE {lossTr:.6f}, test RMSE {lossTe:.6f}\")\n",
    "\n",
    "        if lossTe < bestLossTe:\n",
    "            bestEpoch = iEpoch\n",
    "            bestLossTe = lossTe\n",
    "            torch.save(model.state_dict(), f\"{modelDir}/best_model.pth\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': iEpoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': lossTe\n",
    "                    },\n",
    "                f\"{checkpointDir}/best_model_state.tar\"\n",
    "            )\n",
    "\n",
    "        elif iEpoch - bestEpoch > earlyStopThresh:\n",
    "            progBar.write(f\"Training stop because testing loss has not decreased for {earlyStopThresh} epochs\")\n",
    "            break\n",
    "\n",
    "        lsLossTr.append(lossTr)\n",
    "        lsLossTe.append(lossTe)\n",
    "\n",
    "        # save the losses\n",
    "        loss_data = {\n",
    "            \"training_loss\": lsLossTr,\n",
    "            \"testing_loss\": lsLossTe\n",
    "        }\n",
    "        loss_df = pd.DataFrame(loss_data)\n",
    "        loss_df.to_csv(f\"{lossDir}/losses.csv\")\n",
    "\n",
    "plt.plot(loss_df)\n",
    "plt.title(f\"{version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with second half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batchSz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Data Loader ###\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mbatchSz\u001b[49m}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spliDataset2Half:\n\u001b[1;32m      5\u001b[0m     loaderTrain \u001b[38;5;241m=\u001b[39m DataLoader(DSet(x_train_2, y_train_2), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batchSz' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the first dataset first\n",
    "if spliDataset2Half:\n",
    "    xTrainTs = torch.Tensor(x_train_2)\n",
    "    yTrainTs = torch.Tensor(y_train_2)\n",
    "    xTestTs = torch.Tensor(x_test)\n",
    "    yTestTs = torch.Tensor(y_test)\n",
    "else:\n",
    "    xTrainTs = torch.Tensor(x_train)\n",
    "    yTrainTs = torch.Tensor(y_train)\n",
    "    xTestTs = torch.Tensor(x_test)\n",
    "    yTestTs = torch.Tensor(y_test)\n",
    "\n",
    "### Data Loader ###\n",
    "params = {'shuffle': True, 'batch_size': batchSz}\n",
    "\n",
    "if spliDataset2Half:\n",
    "    loaderTrain = DataLoader(DSet(x_train_2, y_train_2), **params)\n",
    "    loaderTest = DataLoader(DSet(x_test, y_test), **params)\n",
    "else:\n",
    "    print(\"Do not run the following cells.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsLossTr = []\n",
    "lsLossTe = []\n",
    "bestEpoch = 0\n",
    "bestLossTe = 1e6\n",
    "earlyStopThresh = 20\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with tqdm.trange(nEpochs, unit='epoch', mininterval=0) as progBar:\n",
    "\n",
    "    for iEpoch in progBar:\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for xBatchTr, yBatchTr in loaderTrain:\n",
    "            model = model.to(device)\n",
    "            xBatchTr = xBatchTr.to(device)\n",
    "            yBatchTr = yBatchTr.to(device)\n",
    "            yPredBaTr = model(xBatchTr)\n",
    "            lossBaTr = lossFn(yPredBaTr, yBatchTr)\n",
    "            optimizer.zero_grad()\n",
    "            lossBaTr.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluation of Testing\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            model = model.to('cpu')\n",
    "            xTrainTs = xTrainTs.to('cpu')\n",
    "            yTrainTs = yTrainTs.to('cpu')\n",
    "            xTestTs = xTestTs.to('cpu')\n",
    "            yTestTs = yTestTs.to('cpu')\n",
    "            yPredTr = model(xTrainTs)\n",
    "            lossTr = np.sqrt(lossFn(yPredTr, yTrainTs))\n",
    "            yPredTe = model(xTestTs)\n",
    "            lossTe = np.sqrt(lossFn(yPredTe, yTestTs))\n",
    "\n",
    "        progBar.set_description(f\"Epoch {iEpoch}: train RMSE {lossTr:.6f}, test RMSE {lossTe:.6f}\")\n",
    "        progBar.update(1)\n",
    "\n",
    "        if iEpoch % 100 == 0:\n",
    "            progBar.write(f\"Epoch {iEpoch}: train RMSE {lossTr:.6f}, test RMSE {lossTe:.6f}\")\n",
    "\n",
    "        if lossTe < bestLossTe:\n",
    "            bestEpoch = iEpoch\n",
    "            bestLossTe = lossTe\n",
    "            torch.save(model.state_dict(), f\"{modelDir}/best_model_2.pth\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': iEpoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': lossTe\n",
    "                    },\n",
    "                f\"{checkpointDir}/best_model_state_2.tar\"\n",
    "            )\n",
    "\n",
    "        elif iEpoch - bestEpoch > earlyStopThresh:\n",
    "            progBar.write(f\"Training stop because testing loss has not decreased for {earlyStopThresh} epochs\")\n",
    "            break\n",
    "\n",
    "        lsLossTr.append(lossTr)\n",
    "        lsLossTe.append(lossTe)\n",
    "\n",
    "        # save the losses\n",
    "        loss_data = {\n",
    "            \"training_loss\": lsLossTr,\n",
    "            \"testing_loss\": lsLossTe\n",
    "        }\n",
    "        loss_df = pd.DataFrame(loss_data)\n",
    "        loss_df.to_csv(f\"{lossDir}/losses_2.csv\")\n",
    "\n",
    "plt.plot(loss_df)\n",
    "plt.title(f\"{version}\")\n",
    "plt.savefig(f\"{learingCurveDir}/learning_curve_2.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
